# Q-A-Chatbot-powered-by-Ollama

# ü§ñ Simple Q/A Chatbot through Ollama

This is a basic Question and Answer chatbot built using **LangChain**, **Streamlit**, and **Ollama**. It serves as a learning project to understand how to combine LLMs with prompt templates and simple UI for querying models locally.

---

## üß† What It Does

- Takes a user question via a simple Streamlit UI.
- Sends the question to a locally running LLM (via Ollama).
- Formats the prompt using `ChatPromptTemplate`.
- Tracks runs using LangSmith (if API key is set).
- Returns an answer generated by the selected model.

---

## üõ†Ô∏è Tech Stack

- **LangChain** ‚Äì Framework to build LLM-based apps
- **Ollama** ‚Äì Run LLMs locally
- **Streamlit** ‚Äì Web interface
- **Python (.env)** ‚Äì For environment variable management

---


## üöÄ Getting Started

### 1. Clone the repository
git clone https://github.com/yourusername/ollama-chatbot.git
cd ollama-chatbot

## Install dependencies
pip install -r requirements.txt

## Create a .env file
LANGCHAIN_API=your_api_key_here

LANGCHAIN_PROJECT=Simple Q/A Chatbot through Ollama

LANGCHAIN_TRACING_V2=true

# Run the app
streamlit run main.py
